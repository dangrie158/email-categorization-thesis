# Initial Email Labeling

As stated in [Section @sec:intro], @gfireport found that 62% of the people that use email on a daily basis already use different folders to organize their inbox. However, in this chapter methods will be presented and evaluated that can help users that did not yet use this organization technique to sort their emails when a pure manual sorting is not feasible due to the large size of the inbox.

Since the emails are not yet associated with a folder (they have no label / class), the task is to learn these labels in an unsupervised way. One commonality for all approaches that will be presented in this chapter is the general process to get a label for each document:

1. Transform the documents to a vector representation
2. Pool similar documents into a small number of clusters (high tens to low hundreds)
3. Assign a label for each group

In this process, step 1 and 2 are subject to this chapter. Step 3, however, is assumed to be performed by the user. This process therefore helps to reduce the work that needs to be done manually by pre-grouping the documents.

The number of clusters that will be generated by step 2 is a hyperparameter and can be tuned to optimize the tradeoff between the amount of manual work that needs to be done in step 3 and the error rate of step 2, since a too aggressive pooling (too few clusters) will result in more misgrouped documents.

## Article Transformation

As described above, the second step will group documents based on their similarity, therefore the documents need to be transformed into a representation where the distance between them can easily be calculated in a meaningful way. Since this process converts the textual representation of a document into a vector of real values, this process is also called vectorization.

Since clustering algorithms rely on these vectors to be good representations of the documents in the vector space to be able to group similar texts, the choice of the vectorization technique is important for the performance of the complete process.

### TF and TF-IDF Vectorizers

Among the simple vectorization techniques is the term frequency (TF) vectorizer, which assigns each document a vector $\vec { d } =\{ tf_{d,w1},tf_{d,w2},...,tf_{d,wN} \}$ where $tf_{d,wn}$ is the term frequency of the word $wn$ in the document for each of the $N$ words in the vocabulary. A simple extension of the TF vectorizer is the TF-IDF vectorizer, where each $tf_{d,wn}$ term is replaced by the TF-IDF of the word $tf-idf_{d,wn}$.

Since both, the TF and TF-IDF vectorizer, do not order the words according to their appearance in the vectorized document, they both represent the document as a bag of words.

@basavaraju2010novel have successfully demonstrated the usefulness of such simple vectorizers when clustering mail data for the binary classification task spam detection. However, these vector representations use really simple representations for the words. Since each word is only encoded as a *count*, each word can be considered to have the same distance to each other. Therefore, the words `Dog`, `Cat` and `Car` all have the same distance to each other, since their representations in the vector space are all orthogonal.

### Document Vectors from word2vec

Since the objective of word2vec is to learn good vector representations of the words in the corpus, a natural idea is to use these embeddings in the vectorization of the documents.

A simple way to create a document representation from word vectors is to calculate the sum of all word vectors that appear in the document. This yields document vectors whose length are dependent on the number of words in the document. Since the length of a document is generally not an important feature of the document ([@Manning:IntroductionToInformationRetrieval:2008, pp. 129-131]), the vector is also normalized by its length. The vectorization rule is then given by (@sumword2doc) where $K$ is the number of words in the document and $w2v(x)$ is the vector for word $x$ in the word2vec model.

(@sumword2doc) $$ \vec { d } =\frac { 1 }{ K } \sum _{ k=1 }^{ K }{ w2v({ w }_{ d,k }) } $$

Due to the associative property of the summation of the vectors, the document vector loses all information about the order the words appeared in. Therefore, this technique also uses a BOW approach.

### Paragraph Vector

A more sophisticated approach to generate document vectors from word2vec's word vectors is *Paragraph Vector* which was presented as an extension of word2vec by Quoc Le and Thomas Mikolov (-@le2014distributed).

*Paragraph Vector* learns vector representations for text of variable length by using a second projection matrix $D$ alongside of the projection matrix of the words $Wi$ where each paragraph is represented by a row. The matrix $D$ therefore has a dimensionality of $N \times k$ where N is the number of paragraphs in the corpus. The rest of the network uses the same architecture as a normal word2vec model. The output of the first layer is calculated by either concatenating or averaging the row of the document matrix that represents the current paragraph to the normal output of the first layer in a classical word2vec model. The paragraph row of $D$ can be seen as another word that is in every context while learning the paragraph. The row can therefore be seen as a memory that is used for the whole paragraph. Since the number of parameters that need to be learned by the model increases due to the extra paragraph matrix by $N \times k$, however, @le2014distributed point out that learning the extra parameters is still efficient, since the architectural sparsity constraint of only one active row of $D$ at a time.

### Data visualization

Visualizing the output of the vectorization can be a helpful way to determine the quality of the produced vectors. For a quick overview of where the document vectors point to in their embedding vector space and which vectors are neighbours to each other, a two-dimensional representation that can easily be visualized is desirable. However, this requires a dimensionality reduction from the high-dimensional representation vector space to a low dimensional visualization vector space. This dimensionality reduction needs to preserve the general structure of the high-dimensional vectors in their low-dimensional representatives.

One technique that is specifically developed for visualizations of high-dimensional data is *t-Distributed Stochastic Neighbor Embedding* (t-SNE) (@maaten2008visualizing). It provides state-of-the-art, structure-preserving dimensionality reduction and is highly scalable.

[Figure @fig:tsne] shows a visualization of the resulting document vectors using the vectorization techniques described before and a t-SNE dimensionality reduction on the news corpus presented in [chapter @sec:newscorpus]. As one can see, even in the low-dimensional representation shows some correlation between the classes of neighbouring elements which justifies the examination of the quality of clustering algorithms on these vectors.

![Visualization of document vectors created using different vectorization techniques followed by a t-SNE dimensionality reduction. Vectorizers from left to right: TF-IDF, vectors created by the summation of word2vec vectors, Paragraph Vectors](source/figures/word_frequencies.pdf "visualization of document vectors"){#fig:tsne}

## Grouping

To test if the vectorizers are any good, these vectorizers were used for clustering...

### Performance Measure

Homogenity, why?

### Clustering

### LDA Topics

### Conclusiton, Results, some plots for the data

LDA proved to create meaningful topics
word2vec sum was not better, however smaller (200 dims instead of vocab size dims)
