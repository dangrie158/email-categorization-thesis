# Initial Labeling {#sec:initial-labeling}

As stated in [Section @sec:intro], @gfireport found that 62% of the people that use email on a daily basis already use different folders to organize their inbox. However, in this chapter methods will be presented and evaluated that can help users that did not yet use this organization technique to sort their emails when a simple, manual sorting is not feasible due to the large size of the inbox. The labeled corpus can then be used to train the classifiers presented in [chapter @sec:auto-classification], which can the automatically classify all further incoming emails.

Since the emails are not yet associated with a folder (they have no label), the task is to learn these labels in an unsupervised way. One commonality for all approaches that will be presented in this chapter is the general process to get a tag for each document:

1. Transform the articles to a vector representation
2. Pool similar documents into a small number of clusters (high tens to low hundreds)
3. Assign a label for each group

In this process, step 1 and 2 are subject to this chapter. Step 3, however, is assumed to be performed by the user. This process therefore helps to reduce the work that needs to be done manually by pre-grouping the documents. The number of clusters that will be generated by step 2 is a hyperparameter and can be tuned to optimize the tradeoff between the amount of manual work that needs to be done in step 3 and the error rate of step 2, since a too aggressive pooling (too few clusters) will result in more misgrouped documents.

## Document Transformation

As described above, the second step will group documents based on their similarity. Therefore, the documents need to be transformed into a representation where the distance between them can easily be calculated in a meaningful way. Since this process converts the textual representation of a document into a vector of real values, this process is also called vectorization.

The choice of the vectorization technique is essential for the performance of the complete process, since clustering algorithms rely on these vectors to be good representations of the documents in the vector space to be able to group similar texts.

### tf and tf-idf Vectorizers

Among the simple vectorization techniques is the term frequency (tf) vectorizer, which assigns each document a vector $\vec { d } =\{ tf_{d,w1},tf_{d,w2},...,tf_{d,wN} \}$ where $tf_{d,wn}$ is the term frequency of the word $wn$ in the document $d$ for each of the $N$ words in the vocabulary. A simple extension of the tf vectorizer is the tf-idf vectorizer, where each $tf_{d,wn}$ term is replaced by the tf-idf of the word $tf-idf_{d,wn}$. tf-idf is the term frequency divided by the document frequency, the number of documents that contain the term. The document frequency is used as a measure of term specificity(@sparck1972statistical). A term that appears in every document is therefore weighted less than a term only appearing in few documents. Since both, the tf and tf-idf vectorizer, do not order the words according to their appearance in the vectorized document, they both represent the document as a bag of words.

@basavaraju2010novel have successfully demonstrated the usefulness of such simple vectorizers when clustering email data for the binary classification task of spam detection. However, these vector representations use simple representations for the words. Since each word is only encoded as a *count*, it can be considered to have the same distance to all other words in the vocabulary. Therefore, the tokens `Dog`, `Cat` and `Car` all have the same distance to each other, since their representations in the vector space are all orthogonal.

### Document Vectors from word2vec {#sec:clustersum}

Since the objective of word2vec is to learn good vector representations of the words in the corpus, a natural idea is to use these embeddings in the vectorization of the documents. A simple way to create a document representation from word vectors is to calculate the sum of all word vectors that appear in the document. This yields document vectors whose length are dependent on the number of words in the document. Since the length of a document is generally not an important feature of the document ([@Manning:IntroductionToInformationRetrieval:2008, pp. 129-131]), the vector is also normalized to a length of 1. The vectorization rule is then given by (@sumword2doc) where $K$ is the number of words in the document and $w2v(x)$ is the vector for word $x$ in the word2vec model.

(@sumword2doc) $$ \vec { d } =\frac { 1 }{ K } \sum _{ k=1 }^{ K }{ w2v({ w }_{ d,k }) } $$

Due to the associative property of the summation of the vectors, the document vector loses all information about the order the words appeared in. Therefore, this technique also uses a BOW approach.

To take into consideration that not all words have the same significance, the inverse document frequency can be used to weight individual words just like in the tf-idf vector. (@sumword2devidf) shows the vectorization rule with the IDF of each word considered.

(@sumword2devidf) $$ \vec { d } =\frac { 1 }{ K } \sum _{ k=1 }^{ K }{ w2v({ w }_{ d,k }) * idf({ w }_{ d,k }) } $$

### Paragraph Vector

A more sophisticated approach to generate document vectors from word2vec's word vectors is *Paragraph Vector* which was presented as an extension of word2vec by Quoc Le and Thomas Mikolov (-@le2014distributed).

*Paragraph Vector* learns vector representations for a text of variable length by using a second projection matrix $D$ alongside the projection matrix of the words $Wi$ where each paragraph is represented by a row. The matrix $D$ therefore has a dimensionality of $N \times k$ where N is the number of paragraphs in the corpus. The rest of the network uses the same architecture as a normal word2vec model. The output of the first layer is calculated by either concatenating or averaging the row $d_i$ of the document matrix that represents the current paragraph to the usual output of the first layer in a classical word2vec model. The paragraph row of $D$ can be seen as another word that is in every context while learning the paragraph. The row can therefore be seen as a memory that is used for the whole paragraph. The number of parameters that need to be learned by the model increases due to the extra paragraph matrix by $N \times k$, however, @le2014distributed point out that learning the extra parameters is still efficient, since the architectural sparsity constraint of only one active row of $D$ at a time. [Figure @fig:doc2vec] illustrates the general architecture of a neural net to learn Paragraph Vectors by concatenating a document vector from the document matrix $D$ to the input weight matrix of a CBOW word2vec network.

![The structure of a neural network to learn Paragraph Vectors by concatenating them to the input weight matrix Wi](source/figures/doc2vec.pdf "Architecture of a Paragraph Vector network"){#fig:doc2vec}

### Data visualization

Visualizing the output of the vectorization can be a helpful way to determine the quality of the produced vectors. For a quick overview of where the document vectors point to in their embedding vector space and which vectors are neighbors to each other, a two-dimensional representation that can easily be visualized is desirable. However, this requires a dimensionality reduction from the high-dimensional representation vector space to a low-dimensional visualization vector space. This dimensionality reduction needs to preserve the general structure of the high-dimensional vectors in their low-dimensional representatives as good as possible.

One technique that is specifically developed for visualizations of high-dimensional data is *t-Distributed Stochastic Neighbor Embedding* (t-SNE) (@maaten2008visualizing). It provides state-of-the-art, structure-preserving dimensionality reduction and is highly scalable. The used implementation of the algorithm is created by Dmitry Ulyanov and is scalable to multiple CPU cores (-@ulyanov2016multicore).

[Figure @fig:tsne-tfidf] [- @fig:tsne-d2v] show the visualization of the resulting document vectors using the vectorization techniques described before and a t-SNE dimensionality reduction on the news corpus presented in [chapter @sec:newscorpus]. Each category is limited to 500 random documents.  The word2vec vector summation and Paragraph Vector model used the news corpus and Wikipedia corpus presented in the last chapter to learn the word embeddings. As one can see, even in the low-dimensional representation, some correlation between the classes of adjacent elements can be observed which justifies the examination of the quality of the output of clustering algorithms on these vectors. The visualization for the Paragraph Vectors shows only little correlation between the elements of one class; however, this may be due to a latent structure in the set of vectors which causes the t-SNE dimensionality reduction to perform poorly on this set for this task. An indicator for this may be the dense hotspot on the left, containing many vectors in a small area.

![Visualization of document vectors created using tf-idf vectorization and t-SNE dimensionality reduction](source/figures/tsne_tfidf.pdf "Visualization of tf-idf vectors"){width=90% #fig:tsne-tfidf}

![Visualization of document vectors created by the summation of word2vec word vectors](source/figures/tsne_word2vec.pdf "Visualization of word2vec vectors"){width=90% #fig:tsne-w2v}

![Visualization of document vectors created using Paragraph Vectors](source/figures/tsne_doc2vec.pdf "Visualization of Paragraph vectors"){width=90% #fig:tsne-d2v}

## Clustering {#sec:clustering}

Since the visualization of the vectors can only be used as an intuitive way to check if a clustering algorithm may yield good results, this section describes the testing method und results found by applying different clustering algorithms on the vectorized documents.

The following clustering algorithms were used for the evaluation:

- **k-Means** using Mini-Batches
- Agglomerative clustering using **Ward** linkage
- **Birch**

Each algorithm was configured to create 100 clusters which were then evaluated. For this evaluation, the corpus was trimmed so that each category has the same number of documents, thus equalizing the a priori probability. To avoid the removal of too many documents, the smallest category with less than 1000 documents, *Aktuell* was ignored completely.

### Performance Measure

To gauge the quality of the output of a clustering, there are multiple measures; *Homogeneity*, *completeness* and *V-measure* (@rosenberg2007v). A cluster with a homogeneity of 1 contains only elements of a single class. A *complete* cluster contains all elements of a class in the dataset in a single cluster. *V-measure* is the harmonic mean of homogeneity and completeness ( $2*\frac { homogeneity*completeness }{ homogeneity+completeness }$ ). However, since in this application the clusters are only used to pre-group the documents, the completeness of the clusters and therefore also the V-measure is of no importance. Therefore, only the homogeneity is used as a performance measure.

### Results

[Table @tbl:clustering-results] shows the result of the homogeneity performance benchmark on the corpus. The simple summation of word2vec word vectors performed best in this test, even better than the more complex Paragraph Vectors. This result may be due to the way the vectors in Paragraph Vectors are learned. While the word vectors in the projection matrix $Wi$ are adjusted during the complete learning phase and with every document, the vectors for the paragraphs in $D$ only have at most as many training steps as words in the document. For a rather short document, this means that its paragraph vector is only adjusted in a few iterations from its random initial state. Therefore, while Paragraph Vectors can be learned on a text of arbitrary length, the quality of the vectors increases with longer documents.

The bad results for the tf-idf vectors can be explained by the simplicity of the vectorization. While the other vectorizers could profit from the extensive Wikipedia base corpus to learn peculiarities of the language, like tense independence and synonyms, the tf-idf vectorizer can only weight the terms depending on their document frequency. Due to the large dimensionality of the tf-idf vectors equal to the length of the vocabulary, the clustering was also the slowest of all three vectors.

|                 |  tf-idf | word2vec summation | Paragraph Vectors |
|-----------------|---------|--------------------|-------------------|
| k-Means         |   0.099 |          **0.421** |             0.213 |
| Ward            |   0.173 |          **0.471** |             0.202 |
| Birch           |   0.209 |          **0.468** |             0.340 |
Table: Homogeneity of different clustering algorithms on the vectors of the vectorizers {#tbl:clustering-results}

## LDA Topics

Another possibility for clustering an initial corpus may be the LDA topic model presented in [Chapter @sec:lda]. Therefore, this chapter will introduce a way to use the output of the LDA model to assign each document to one of 100 clusters.

For these tests, a topic model with 100 topics was learned on the stopword filtered news corpus using gensim's implementation of LDA [^gensim-lda], thus describing each document as a mixture of 100 topics. These representations of the documents could then be used to assign each document to the topic which has the highest share in the mix of topics. This simple technique already yielded a homogeneity of 0.5013, which is better than any of the clustering techniques. By calculating the average distance between the topic with the highest and the second highest share, one could observe that documents which were associated with a class that is a minority in the current cluster, have a lower difference between the first two topics with the biggest shares ([Figure @fig:avg-distance-lda]).

This observation leads to the assumption that documents were misgrouped, when the LDA model represented them by a mix of topics where two or more topics, one of which is the biggest part, have nearly the same share. However, using the variance over all topics in the representation, no such correlation could be found ([Figure @fig:avg-variance-lda]).

![Average difference between the two biggest shares in a documents topics](source/figures/lda-avg-dist.pdf "Difference between LDA topic shares"){width=80% #fig:avg-distance-lda}

![Average variance between the shares of all topics](source/figures/lda-avg-var.pdf "Variance in LDA topics"){width=80% #fig:avg-variance-lda}

This observation led to the introduction of a *minimum confidence* parameter. This parameter specifies the minimum distance between the two largest topics in the document before a document gets assigned to the biggest topic's cluster. If the distance between the two topics is less than this value, the document gets assigned to a special cluster for unassigned documents. This cluster is not considered during the homogeneity measure, since a user could easily sort each mail in this cluster manually after sorting all other clusters. Collecting these documents, however, still provides an advantage to the user as long as the number of documents in this cluster is relatively low. [Figure @fig:confidence-parameter] shows the effect of the *minimum confidence* parameter on the homogeneity measure and the number of documents in the special cluster.

As one can see, the rate growth of the special cluster when increasing the *minimum confidence*  does not justify the small increase in homogeneity.

![Correlation between average homogeneity and number of unclustered elements. The confidence parameter is varied in the interval (0, 0.035)](source/figures/confidence-parameter.pdf "Effect of the confidence parameter"){width=80% #fig:confidence-parameter}

To show the effect of the number of classes in the ground truth, the clustering was repeated using only 4 classes; *Politik*, *Sport*, *Technologie* and *Kultur*. The choice of categories was based on no particular reason, however it may be noteworthy that all are very distinct topics from one another. With only these classes, the homogeneity of the 100 clusters reached 71.46%.

[^gensim-lda]: https://radimrehurek.com/gensim/models/ldamodel.html

### LDA vectorization

LDA maps each input document to a mix of topics. The output can therefore also be seen as a vector with dimensionality equal to the number of topics. This LDA vectorization was also tested using the same clustering algorithms that were used in [Chapter @sec:clustering]. The results are presented in [Table @tbl:lda-clustering].

All results show a worse performance regarding the homogeneity of the clusters than the assignment to the biggest topics cluster. This decrease in performance when all dimensions of the vector are taken into account may be due to the fact that, as shown above, the document is mainly represented by the largest component. The LDA model, however, yields a dense vector with non-zero components for most topics. The complete vector therefore contains noise.

|                 |     homogeneity |
|-----------------|-----------------|
| k-Means         |           0.276 |
| Ward            |           0.266 |
| Birch           |           0.242 |
Table: Homogeneity of the clusters created using LDA as a vectorizer {#tbl:lda-clustering}

## Conclusion and Discussion

The results of the clustering experiment show, that the summation of word vectors yields a good representation for a document. It is also visible that the vectorization using a model which was pre trained on a large corpus to learn peculiarities of the language can generate better vectors with a rather low dimension than classical tf-idf vectorization which yields high dimensional but sparse vectors.

The LDA model proved to create meaningful topics, since one could cluster the documents with the best result in all experiments just by using the topic that contributed the most to the document, effectively high-pass filtering the vector elements. However, using the complete, unfiltered vector decreased the performance, probably due to noise introduced by the smaller components in the vector.

The best result was a homogeneity of just over 50% for 100 clusters and 10 classes in the ground truth. By reducing the number of categories to 4 while keeping all other parameters, the homogeneity could reach over 71%. Especially for the categorization into fewer classes, this technique can provide the user with a big reduction in workload when initially sorting a inbox into multiple folders. However, any other algorithm that is trained on a corpus using the labels provided by this technique can, at best, only reach an accuracy equal to the homogeneity of the clusters.
