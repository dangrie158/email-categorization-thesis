# Automatic Classification

In this chapter, different approaches to automatically classify new emails using word2vec VSMs will be presented and evaluated. The evaluation also compares the results with state-of-the-art text classification techniques, for example support vector machines (SVM) or random forests (RF).

All algorithms are trained and evaluated on the news corpus with a 90% / 10% split for training and evaluation data. The corpus is split so that the a priori probability for all classes is equal in the validation and training set.

Due to the small size of the category ```Aktuell```, this category was not considered during the evaluation since it would only contain ~20 validation elements and is, with its size, an outlier to all other categories. Furthermore the category ```Ausland``` was also removed from the evaluation since it proved to contain a wide mix of topics concerning politics, sports, society and economy, all also present in other categories. This leaves 9 distinct classes for the evaluation.

All algorithms, except for the CNN, were evaluated on a 2015 MacBook Pro with a 2.9 GHz i5 Processor and 16 GB of memory.

## Classification through Inversion via Bayes Rule

@taddy2015document shows how any distributed language model can be used as a classifier by using the Bayes Rule.

The word2vec model introduced in [Chapter @sec:word2vec] learns a projection matrix $Wi$ of the high-dimensional vocabulary space into a low-dimensional embedding space by maximizing the log-likelihood either by predicting a context from the current word (skip-gram) or by predicting the word given a context (CBOW). During training, the input projection matrix $Wi$ and the output projection matrix $Wo$ get adjusted to minimize the loss function. After training, however, the same architecture can be used to calculate the likelihood of any new word-context pair to be generated by the model.

By training a separate model $\upsilon_y$ for each of the C classes in the training set ($y \in \{1, \dots, C\}$), one can specify the likelihood of a document $d = \{ w_1, \dots, w_d \}$ as the *composite likelihood* of all its words in the model:

(@comp-likelihood) $$ log\,p(d|y)=\sum_{ w\in d }^{  }{ log\,p\upsilon_y (w) } $$

Then, the Bayes rule can be applied to inverse this likelihood to get the posterior probability;

(@bayes-rule) $$ p(y|d)=\frac { p(d|y)\pi_{ y } }{ \sum_{ c\in C\setminus y }^{  }{ p(d|c)\pi_{ c } } } $$

with $\pi_c = \frac{1}{C}$ being the prior for class $c$.

The classification rule is then the maximization of the posterior probability which is equal to the maximization of the likelihood (@classification-rule).

(@classification-rule) $$
\begin{aligned}
\bar { y }  &= \underset { y }{ argmax } (p(y|d)) \\
            &= \underset { y }{ argmax } (p(d|y))
\end{aligned}
$$

### Practical Implementation {#sec:w2vpractical}

The likelihood of a new document, that was not present in the training data, can be calculated by the ```score``` function of gensim's word2vec implementation. Therefore, the classifier described above is trivial to implement.

A practical implementation of the classifier, especially for a higher number of classes, however, comes with some challenges. To overcome the problem of corpus size and the initial, random state described in [Chapter @sec:wikipedia-corpus], a model on the wikipedia corpus was trained, which then could be used as a neutral base language model for each category's training data. This base model with $V=1,862,009$ unique words, when configured to embed the words in a $k=200$ dimensional vector space, has a memory consumption of $k \times V \times b$ for both, the $Wi$ and the $Wo$ matrix where $b$ is the size of the datatype used to save the elements of the matrix. Gensim uses the ```numpy.float32``` data type which needs 4 bytes of memory. Therefore, only the two projection matrices need approximately 3GB.

The mapping of the words to the corresponding row, respectively column of the projection matrices, the dictionary, can not be calculated this precisely, since the words vary in length.

A 200-dimensional model that uses hierarchical softmax needs in total just over 3.4 GB, therefore the dictionary and huffman tree occupy around 400MB. The dimensionality of the model affects the size of the projection matrices in a linear way, however it does not affect the dictionary nor the other parts of the model. A 400-dimensional model of the same corpus therefore requires around 6.4 GB.

The size of this base model changes only little when learning the specific training data for each class. This is due to the fact that most words in the training data are already present in the base model's corpus and therefore training these only adjusts the elements in the matrices. Only words not already present add new rows, respectively columns, to the projections which proved to be a negligible minority of words.

Still, for each class a modifiable copy of the base model is needed. For the evaluation with 9 classes, the total size of all models is ~57.6 GB when using the 400-dimensional configuration or 30.6 GB for the 200-dimensional models respectively. Since this amount of memory was not available on the evaluation machine, the models were loaded sequentially, then the likelihood of each document $p(y|d)$ for all classes $y$ in the validation set was calculated for the model. The likelihoods were collected and the classification rule (@classification-rule) was applied after all documents were *scored* with each model.

### Derived word2vec Models

The practical implementation described in [Chapter @sec:w2vpractical] either requires large amounts of memory or does only allow for one or few models to be loaded in-memory at the same time. Both options may be suitable for a experimental environment, not, however, for a classifier used in a product.

Learning a more specific model on top of a base model however, only affects a relatively small fraction of the base model's parameters, depending on the number of different words in the specific training data. To leverage this property, an implementation of word2vec for derived models was written, based on gensim's implementation.

This implementation provides a ```DerivedWord2Vec``` class which expects a ```base_model``` parameter in it's constructor. The class implements the same interface as gensim's ```Word2Vec``` class for learning, scoring and model persistency.

While learning however, the implementation performs a copy-on–write (COW) of the projection matrices and the dictionary of the base model. Therefore, while learning, double of the base models size is required as allocated and used memory. To leverage the fast C implementation of the word2vec learning routines, each new batch of training documents is learned on the base model. After learning a batch of documents, the changes in the base model are copied to the derived model by comparing all parameters with the copy created before learning. Then, the original state of the base model is restored.

This implementation does not allow for concurrent training of multiple derived models at the same time, since the base model is shared between all derived models and is changed while training. However, since gensim's word2vec implementation can utilize multiple cores while learning, there is no need to concurrently learn multiple models on the same base model for training speed reasons.

During the projection of words into the embedding space or scoring of documents, the copied differences found while learning are used to shadow the lookup of parameters in the base model. This means lookup is first performed on the sparse representation of derived parameters. On a miss, the lookup is performed on the dense base model parameters. Since the base model is in this case used read-only, concurrent use of multiple derived models on the same base model is possible.

Using this implementation, a derived model for each category in the news corpus was learned. The size of each model is shown in [Table @tbl:derived-sizes]. Using a separate word2vec model for each category required ~$3.4 GB * num_categories$ of memory while using derived models need $3.4GB + \sum { size\_ of\_ model }$. Therefore, the amount of memory required drops significantly when using more than one specific model at the same time. The complete classifier with all derived category models loaded requires TODO GB of memory, compared to the 57.6 GB for the separate models.

|                  | Size [MB] |
|------------------|-----------|
| Wikipedia Corpus | 3400      |
|                  |           |
|                  |           |
Table: Sizes of the derived and complete model for each category {#tbl:derived-sizes}


- riesen modelle
- beste performance
- schnell (gute performance für lernen und sehr gute performance für kategorisieren)

## word2vec as doc2vec (SVD as classifier)

- schlechte performance (schlechter als tf-idf vectorizer mit SVD)
- nur ein riesen modell
- semi schnell (SVD lernen ok, prediction gut)

## cnn
- (noch) beschissene performance
- mini modell (cnn) + 1 w2v modell
- nicht alle daten können verwendet werden (a-priori normalisierung)
- langsam (ewig am lernen, langsame vorhersage)

problems:
- random initialization (missing words) needs same distribution
- padding (how avoided) (maybe: is it really a problem when worde repititions from article beginning)
- same a-priori (overfitting to a-priori as parameter)
    - solved through equalization of a-priori
- hard to learn (all other models were learned on a macbook, this needs to be learned on a 4GPU cluster)

## results & comparison of methods
not near 100% accuracy (currently ~80%)
but: most methods have a "confidence" e.g. log-likelihood
-> confidence score
if confidence < minimum score: don't sort the email

speed / mem consumption: auch wenn initiales lernen ein "einmaliger" task ist ist geschwindigkeit immer gut. ausserdem müssen modelle eventuell neu gelert werden (z.B. wenn neue klassen hinzukommen oder andere hyperparameter verändert werden sollen)

## evaluation / results

## other metrics (input parameters)
- metadata (auch wenn keine tests damit gemacht wurden weil der corpus fehlt ein paar gedanken dazu)
- author (via SVD)
- how to combine
- maybe sometimes only the metadata is available (secured connections SMTP over SSL)

experiment:
learn authors -> category and test result (are the authors really in this category)

## Naiver Bayes auf LDA


## random Indexing
