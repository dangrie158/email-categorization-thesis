# Automatic Classification

In this chapter, different approaches to automatically classify new emails using word2vec VSMs will be presented and evaluated. The evaluation also compares the results with state-of-the-art text classification techniques, for example support vector machines (SVM) or random forests (RF).

All algorithms are trained and evaluated on the news corpus with a 90% / 10% split for training and evaluation data. The corpus is split so that the a priori probability for all classes is equal in the validation and training set.

Due to the small size of the category ```Aktuell```, this category was not considered during the evaluation since it would only contain ~20 validation elements and is, with its size, an outlier to all other categories. Furthermore the category ```Ausland``` was also removed from the evaluation since it proved to contain a wide mix of topics concerning politics, sports, society and economy, all also present in other categories. This leaves 9 distinct classes for the evaluation.

All algorithms, except for the CNN, were evaluated on a 2015 MacBook Pro with a 2.9 GHz i5 Processor and 16 GB of memory.

## Classification through Inversion via Bayes Rule

@taddy2015document shows how any distributed language model can be used as a classifier by using the Bayes Rule.

The word2vec model introduced in [Chapter @sec:word2vec] learns a projection matrix $Wi$ of the high-dimensional vocabulary space into a low-dimensional embedding space by maximizing the log-likelihood either by predicting a context from the current word (skip-gram) or by predicting the word given a context (CBOW). During training, the input projection matrix $Wi$ and the output projection matrix $Wo$ get adjusted to minimize the loss function. After training, however, the same architecture can be used to calculate the likelihood of any new word-context pair to be generated by the model.

By training a separate model $\upsilon_y$ for each of the C classes in the training set ($y \in \{1, \dots, C\}$), one can specify the likelihood of a document $d = \{ w_1, \dots, w_d \}$ as the *composite likelihood* of all its words in the model:

(@comp-likelihood) $$ log\,p(d|y)=\sum_{ w\in d }^{  }{ log\,p\upsilon_y (w) } $$

Then, the Bayes rule can be applied to inverse this likelihood to get the posterior probability;

(@bayes-rule) $$ p(y|d)=\frac { p(d|y)\pi_{ y } }{ \sum_{ c\in C\setminus y }^{  }{ p(d|c)\pi_{ c } } } $$

with $\pi_c = \frac{1}{C}$ being the prior for class $c$.

The classification rule is then the maximization of the posterior probability which is equal to the maximization of the likelihood (@classification-rule).

(@classification-rule) $$
\begin{aligned}
\bar { y }  &= \underset { y }{ argmax } (p(y|d)) \\
            &= \underset { y }{ argmax } (p(d|y))
\end{aligned}
$$

### Practical Implementation {#sec:w2vpractical}

The likelihood of a new document, that was not present in the training data, can be calculated by the ```score``` function of gensim's word2vec implementation. Therefore, the classifier described above is trivial to implement.

A practical implementation of the classifier, especially for a higher number of classes, however, comes with some challenges. To overcome the problem of corpus size and the initial, random state described in [Chapter @sec:wikipedia-corpus], a model on the wikipedia corpus was trained, which then could be used as a neutral base language model for each category's training data. This base model with $V=1,862,009$ unique words, when configured to embed the words in a $k=200$ dimensional vector space, has a memory consumption of $k \times V \times b$ for both, the $Wi$ and the $Wo$ matrix where $b$ is the size of the datatype used to save the elements of the matrix. Gensim uses the ```numpy.float32``` data type which needs 4 bytes of memory. Therefore, only the two projection matrices need approximately 3GB.

The mapping of the words to the corresponding row, respectively column of the projection matrices, the dictionary, can not be calculated this precisely, since the words vary in length.

A 200-dimensional model that uses hierarchical softmax needs in total just over 3.4 GB, therefore the dictionary and huffman tree occupy around 400MB. The dimensionality of the model affects the size of the projection matrices in a linear way, however it does not affect the dictionary nor the other parts of the model. A 400-dimensional model of the same corpus therefore requires around 6.4 GB.

The size of this base model changes only little when learning the specific training data for each class. This is due to the fact that most words in the training data are already present in the base model's corpus and therefore training these only adjusts the elements in the matrices. Only words not already present add new rows, respectively columns, to the projections which proved to be a negligible minority of words.

Still, for each class a modifiable copy of the base model is needed. For the evaluation with 9 classes, the total size of all models is ~57.6 GB when using the 400-dimensional configuration or 30.6 GB for the 200-dimensional models respectively. Since this amount of memory was not available on the evaluation machine, the models were loaded sequentially, then the likelihood of each document $p(y|d)$ for all classes $y$ in the validation set was calculated for the model. The likelihoods were collected and the classification rule (@classification-rule) was applied after all documents were *scored* with each model.

### Partial word2vec Models

The practical implementation described in [Chapter @sec:w2vpractical] either requires large amounts of memory or does only allow for one or few models to be loaded in-memory at the same time. Both options may be suitable for a experimental environment, not, however, for a classifier used in a product.

To reduce the amount of memory required,


- riesen modelle
- beste performance
- schnell (gute performance für lernen und sehr gute performance für kategorisieren)

## word2vec as doc2vec (SVD as classifier)

- schlechte performance (schlechter als tf-idf vectorizer mit SVD)
- nur ein riesen modell
- semi schnell (SVD lernen ok, prediction gut)

## cnn
- (noch) beschissene performance
- mini modell (cnn) + 1 w2v modell
- nicht alle daten können verwendet werden (a-priori normalisierung)
- langsam (ewig am lernen, langsame vorhersage)

problems:
- random initialization (missing words) needs same distribution
- padding (how avoided) (maybe: is it really a problem when worde repititions from article beginning)
- same a-priori (overfitting to a-priori as parameter)
    - solved through equalization of a-priori
- hard to learn (all other models were learned on a macbook, this needs to be learned on a 4GPU cluster)

## results & comparison of methods
not near 100% accuracy (currently ~80%)
but: most methods have a "confidence" e.g. log-likelihood
-> confidence score
if confidence < minimum score: don't sort the email

speed / mem consumption: auch wenn initiales lernen ein "einmaliger" task ist ist geschwindigkeit immer gut. ausserdem müssen modelle eventuell neu gelert werden (z.B. wenn neue klassen hinzukommen oder andere hyperparameter verändert werden sollen)

## evaluation / results

## other metrics (input parameters)
- metadata (auch wenn keine tests damit gemacht wurden weil der corpus fehlt ein paar gedanken dazu)
- author (via SVD)
- how to combine
- maybe sometimes only the metadata is available (secured connections SMTP over SSL)

experiment:
learn authors -> category and test result (are the authors really in this category)

## Naiver Bayes auf LDA


## random Indexing
