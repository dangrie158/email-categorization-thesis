# Related Work

## Tools

scipy
Gensim
Keras

mention von sasha can's paper

This section provides an overview over the technologies and algorithms used to implement and validate the classifiers presented in the next section.

## Word2Vec

Word2vec is a method to build a Vector Space Model (VSM) in which words can be represented (embedded) which was introduced by [@mikolov2013efficient].
Classical NLP models often use simple bag of words (BOW) approaches to handle words, where each word is treated as a single, atomic symbol. The words then are represented as a numeric ids (e.g. ```car: 350``` and ```truck: 543```) or a single-hot vector. A single-hot vector is a binary sparse vector with the same dimensionality as the length of the vocabulary with only one (single) non-zero (hot) element. These simple BOW approaches however, do not allow the representation of similarities between words (e.g. the word ```car``` and ```truck``` are both motorized transportation devices with 4 or more wheels). Therefore an algorithm using one of these representations cannot take advantage from this knowledge unless it learns them as part of the training, which requires more training data.

Word2vec however tries to embed each word in the vocabulary in a vector space with, compared to the dimensionality of single-hot encodings, very low dimensionality (typically between 100 - 1000). It uses the *distributional hypothesis* which states that words which occur in the same context tend to share a similar meaning [@harris1954distributional]. The algorithm therefore tries to learn dense vectors that have a high (cosine-) similarity for words that cooccure often and a low similarity for words that do not occur in the same context.

Word2Vec uses a *predictive method* that tries to directly predict a word using it's context as an input. Another approach is to use a *count based* model like Latent Semantic Indexing (LSI) that tries to learn the correlation between words by counting the cooccurence and then transform this information into a low-dimensional vector space using Single Value Decomposition (SVD) [@dumais1988using]. However, both, predictive and count based methods can be learned unsupervised on unlabeled training data because the only input is the context of the current word. This allows to use any text corpus in any language where the distributional hypothesis holds true. To train word2vec models with good vector representations\footnote{good vector representation of words will have a small distance for words with a similar meaning and big distances for words with different meanings}, a big corpus is needed. This is due to the fact that the distributional hypothesis is a statistical model which profits from a big corpus where words occur in their context more than once. Mikolov et al. published a pre-trained word2vec model they used to evaluate the optimization methods in @mikolov2013distributed. This model was trained on 100 billion (english) words from a google news corpus.

### Learning of the Word Embeddings

Word2Vec uses one of two approaches to learn vector representations of the words in the corpus, skip-gram or continuous bag of words (CBOW).

The CBOW approach tries to learn the model in a way that it maximizes the probability to generate the current target word out of a bag of context words. Due to the the bag of words approach for the context words, the order of the words in the current context don't matter. Since CBOW uses multiple input words, the input to the model is either generated by simply summing the vectors for all context words or by using the mean average of all input words.

![Continuous bag of words architecture with a context window of 4 words and a summing strategy to combine the multiple input vectors](source/figures/cbow-aritechture.pdf "continuous bag of words architecture"){#fig:cbow}

In contrast to the CBOW approach, the skip-gram architecture tries to predict the context words out of the current word as input. The order of the words is weighted into the target by giving context words closer to the current word a higher weight.

![Skip-gram architecture with a context window of 4 words](source/figures/skipgram-aritechture.pdf "skipgram architecture"){#fig:skipgram}

On the Google Code page for the word2vec project\footnote{https://code.google.com/archive/p/word2vec/} the authors state that CBOW is the faster algorithm whereas skip-gram provides a better performance for infrequent words. Since in this thesis only models are used that use the CBOW architecture, the following section will explain how the vector representations are learned in this configuration.

The model that will be learned to optimize the CBOW objective is a neural net with one, fully connected, hidden layer. The input and output layer have the same dimensionality $v$ which is equal to the size of the vocabulary $V$. The hidden layer has dimensionality $k$ which equals the dimensionality of the vector space where the words will be embedded. Since the layers are fully connected the weights of the connections between each layer can be represented in a matrix ${Wi}_{v\times k}$ for the connection between input and hidden layer or ${Wo}_{k\times v}$ for the connections from the hidden to the output layer respectively.

![The structure of a neural net with a CBOW architecture](source/figures/cbow-nn.pdf "detail view of CBOW net"){#fig:cbowdetail}

As shown in [Figure @fig:cbow] above, to train the the word ${w}_{t} \in V$ the context $c_{ { w }_{ t } }=\{ { w }_{ t-s },{ w }_{ t-C-1 },\dots { w }_{ t-1 },{ w }_{ t+1 },\dots ,{ w }_{ t+C }\}$ is considered relevant, where $C$ is half the window size and ${ w }_{ x }$ is the $x$-th word in the (sorted) vocabulary represented as a one-hot vector. To represent the context $c_{ { w }_{ t } }$, the single-hot vectors of each word in the context is either summed up or mean-averaged ((@sumcontext) or (@meancontext) respectively).

(@sumcontext) $$ \vec { { c }_{ { w }_{ t } } } =\sum _{ w\in { c }_{ { w }_{ t } } }^{  }{ w }  $$

(@meancontext) $$ \vec { { c }_{ { w }_{ t } } } =\frac { 1 }{ 2C } \sum _{ w\in { c }_{ { w }_{ t } } }^{  }{ w }  $$

The hidden layer has a simple, linear activation function ($f(x) = x$). Therefore the output of the hidden layer is simply the result of multiplying the input vector $\vec { { c }_{ { w }_{ t } } }$ with the weight matrix $Wi$.

(@hiddenlayercalc) $$ {\vec{h} = \vec { { c }_{ { w }_{ t } } } \times Wi = \frac { 1 }{ 2C }  \sum _{ w\in { c }_{ { w }_{ t } } }^{  }{ { Wi }_{ w } }}^{T}$$

Since the rows of the input matrix correspond to the words in the vocabulary this multiplication can be implemented very efficiently by summing up the rows of the matrix where the input vector has a non-zero value and transpose the result (@hiddenlayercalc). In case of mean-averaged input vectors the result also needs to be divided by the context size.


### other parameters / optimization techniques
- window
- subsamping (like presented in @mikolov2013distributed)
- negative sampling
- hierarchical softmax

### why word2vec is cool
- unsupervised learn connectins between words
- unsupervied learn relations between words (city -> capital like in @mikolov2013distributed)


- wordembeddings to learn beziehungen zwischen w√∂rtern


## SVD
used many times (successfully) in NLP categorization tasks (ref to chap. 5 with results (SVD TF-IDF second best))

## Latent Ditrichlet Allocation
