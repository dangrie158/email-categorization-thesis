# Final Conclusion

In this thesis, word embeddings, mainly based on word2vec, were evaluated for different natural language processing tasks against state-of-the-art technologies.

In [chapter @sec:initial-labeling], texts are clustered using a number of different document vectors. While the vectors built using a word2vec model performed significantly better than the TF-IDF document vectors, the best performance could be achieved by using an LDA topic model. With the overall homogeneity of the clusters on a ground truth with 11 classes being only slightly better than 50%, this technique may not be suitable for complex corpora.
However, the clustering may still be useful in conjunction with other techniques, for example as a tie-breaker in a rule based approach.

The results from [chapter @sec:auto-classification] provide evidence that the classifiers using the dense word vectors learned on a base corpus perform better than classifiers using TF-IDF vectors, especially when only little training data is available. Although the SVC using TF-IDF vectors performed best on both corpora when learned on the complete training set, the better performance over a wide range of training set sizes is a clear advantage of the classifiers using a word2vec model. The good scalability of the likelihood maximization classifier and the ability to introduce new classes without relearning makes this classifier best suited for the task of email classification. The memory efficient implementation of derived word2vec models also made this classifier usable in practice. Although a naive Bayes classifier also allows the introduction of new classes at any time, the performance was worse except for scenarios where very little training data is available.

[Chapter @sec:new-classes] introduced a method to further increase the performance of a naive Bayes classifier when only very little training data is available by creating more data using a word2vec model. This scenario is very important for the live classification of emails since new classes can be introduced at any time. This chapter also introduced a implementation of an algorithm to split compound words in synthetic, fusional languages.

The evaluation of the algorithms in all scenarios show a clear advantage of the word embeddings over a simple TF-IDF vectorization. Also the properties of word2vec vectors can be used to gain more information about a document than just the words it contains. The unsupervised nature of word2vec allows the use of large natural language corpora to learn peculiarities of a language and connections between words.

While this thesis did treat every email as a seperate document with no connection to other documents, in reality emails often are part of a longer thread that is about a single topic and therefore could be classified as a whole. Also emails contain a lot of meta information in their header, for example the sender, recipient or the subject. This meta information was also ignored in this thesis. How this information can be used in conjunction with the email's body is subject to further research.
